{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c91fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_scatter\\_version_cuda.pyd\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_geometric\\typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_cluster\\_version_cuda.pyd\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_spline_conv\\_version_cuda.pyd\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_sparse\\_version_cuda.pyd\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, OWL, RDFS\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed9bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded C:\\Users\\yanpe\\OneDrive - Metropolia Ammattikorkeakoulu Oy\\Research\\MD2MV\\data\\TTL\\01ARK\\ARK_MET.ttl\n",
      "Loaded C:\\Users\\yanpe\\OneDrive - Metropolia Ammattikorkeakoulu Oy\\Research\\MD2MV\\data\\TTL\\sensors_linked.ttl\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "ttl_ark = r\"C:\\Users\\yanpe\\OneDrive - Metropolia Ammattikorkeakoulu Oy\\Research\\MD2MV\\data\\TTL\\01ARK\\ARK_MET.ttl\"\n",
    "ttl_sensor = r\"C:\\Users\\yanpe\\OneDrive - Metropolia Ammattikorkeakoulu Oy\\Research\\MD2MV\\data\\TTL\\sensors_linked.ttl\"\n",
    "\n",
    "# 1. Load Data\n",
    "g = rdflib.Graph()\n",
    "files = [ttl_ark, ttl_sensor] \n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        g.parse(file, format=\"turtle\")\n",
    "        print(f\"Loaded {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "# Define Namespaces\n",
    "BRICK = Namespace(\"https://brickschema.org/schema/Brick#\")\n",
    "BOT = Namespace(\"https://w3id.org/bot#\")\n",
    "INST = Namespace(\"https://lbd.example.com/\")\n",
    "PROPS = Namespace(\"http://lbd.arch.rwth-aachen.de/props#\")\n",
    "\n",
    "PREFIXES = {\n",
    "    \"brick\": BRICK, \"bot\": BOT, \"inst\": INST, \"rdfs\": RDFS, \"props\": PROPS\n",
    "}\n",
    "\n",
    "for p, ns in PREFIXES.items(): g.bind(p, ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a356c51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw Triples Sample ---\n",
      "https://lbd.example.com/wall_ae67b2bf-8316-4d92-a0e6-1ed84e5b3bb3  |  http://lbd.arch.rwth-aachen.de/props#globalIdIfcRoot_attribute_simple  |  2kPxA$WnPDag3c7jXEMpkp\n",
      "https://lbd.example.com/wall_1798ccea-aa19-41f7-b372-ad96d543e26c  |  https://w3id.org/bot#hasSubElement  |  https://lbd.example.com/door_fee3f249-9668-4076-ae9d-60d0eae2725e\n",
      "https://lbd.example.com/railing_81c21791-fb6b-40af-80ed-189e6e37faac  |  http://lbd.arch.rwth-aachen.de/props#objectTypeIfcObject_attribute_simple  |  Railing:KÃ¤sijohde d 30mm\n",
      "https://lbd.example.com/wall_7c575b5f-b6bb-4516-801b-e3e04fadb854  |  http://www.w3.org/2002/07/owl#sameAs  |  https://lbd.example.com/IfcWallStandardCase_14633015\n",
      "https://lbd.example.com/flowterminal_9c247209-b941-4d40-af98-a82fda27464b  |  http://lbd.arch.rwth-aachen.de/props#batid_attribute_simple  |  14626612\n",
      "\n",
      "--- Found Node Types (10) ---\n",
      "{'https://w3id.org/bot#Site', 'https://w3id.org/bot#Storey', 'https://w3id.org/bot#Building', 'https://brickschema.org/schema/Brick#Room_Air_Temperature_Sensor', 'https://w3id.org/bot#Space', 'https://brickschema.org/schema/Brick#Humidity_Sensor', 'http://www.w3.org/2002/07/owl#ObjectProperty', 'https://w3id.org/bot#Element', 'https://brickschema.org/schema/Brick#CO2_Sensor', 'http://www.w3.org/2002/07/owl#DatatypeProperty'}\n",
      "\n",
      "--- Found Relations (25) ---\n",
      "{'http://lbd.arch.rwth-aachen.de/props#elevationIfcBuildingStorey_attribute_simple', 'http://lbd.arch.rwth-aachen.de/props#descriptionIfcRoot_attribute_simple', 'https://brickschema.org/schema/Brick#isPointOf', 'https://w3id.org/bot#hasSubElement', 'http://www.w3.org/2000/01/rdf-schema#label', 'http://lbd.arch.rwth-aachen.de/props#riserHeightIfcStairFlight_attribute_simple', 'https://w3id.org/bot#containsElement', 'https://w3id.org/bot#hasSpace', 'http://lbd.arch.rwth-aachen.de/props#globalIdIfcRoot_attribute_simple', 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type', 'http://lbd.arch.rwth-aachen.de/props#treadLengthIfcStairFlight_attribute_simple', 'http://lbd.arch.rwth-aachen.de/props#overallWidthIfcWindow_attribute_simple', 'https://w3id.org/bot#hasBuilding', 'http://lbd.arch.rwth-aachen.de/props#batid_attribute_simple', 'https://w3id.org/bot#hasStorey', 'http://lbd.arch.rwth-aachen.de/props#objectTypeIfcObject_attribute_simple', 'http://www.w3.org/2002/07/owl#sameAs', 'http://lbd.arch.rwth-aachen.de/props#numberOfRiserIfcStairFlight_attribute_simple', 'http://lbd.arch.rwth-aachen.de/props#numberOfTreadsIfcStairFlight_attribute_simple', 'http://www.w3.org/2000/01/rdf-schema#comment', 'http://lbd.arch.rwth-aachen.de/props#refElevationIfcSite_attribute_simple', 'http://lbd.arch.rwth-aachen.de/props#overallHeightIfcWindow_attribute_simple', 'http://lbd.arch.rwth-aachen.de/props#longNameIfcSpatialStructureElement_attribute_simple', 'http://lbd.arch.rwth-aachen.de/props#overallHeightIfcDoor_attribute_simple', 'http://lbd.arch.rwth-aachen.de/props#overallWidthIfcDoor_attribute_simple'}\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 triples to see the raw format\n",
    "print(\"--- Raw Triples Sample ---\")\n",
    "for s, p, o in list(g)[:5]:\n",
    "    print(f\"{s}  |  {p}  |  {o}\")\n",
    "\n",
    "# Print all unique Types found\n",
    "unique_types = set()\n",
    "for s, p, o in g.triples((None, RDF.type, None)):\n",
    "    unique_types.add(str(o))\n",
    "print(f\"\\n--- Found Node Types ({len(unique_types)}) ---\")\n",
    "print(unique_types)\n",
    "\n",
    "# Print all unique Predicates found\n",
    "unique_preds = set()\n",
    "for s, p, o in g:\n",
    "    unique_preds.add(str(p))\n",
    "print(f\"\\n--- Found Relations ({len(unique_preds)}) ---\")\n",
    "print(unique_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22c8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered 54966 nodes across 8 types.\n",
      "Graph Construction Complete.\n",
      "Nodes: 54966\n",
      "Edges: 60592\n",
      "Found Edge Types: [('Element', 'hasSubElement', 'Element'), ('Storey', 'containsElement', 'Element'), ('Storey', 'hasSpace', 'Space'), ('Space', 'containsElement', 'Element'), ('Room_Air_Temperature_Sensor', 'isPointOf', 'Space'), ('CO2_Sensor', 'isPointOf', 'Space'), ('Humidity_Sensor', 'isPointOf', 'Space'), ('Building', 'hasStorey', 'Storey'), ('Site', 'hasBuilding', 'Building')]\n"
     ]
    }
   ],
   "source": [
    "# 2. Map URIs to Types (The \"Node Registry\")\n",
    "uri_to_type = {}   # URI string -> Type string (e.g. 'Space')\n",
    "type_to_id = {}    # Type string -> {URI string: int_id}\n",
    "id_to_uri = {}     # Type string -> {int_id: URI string}\n",
    "\n",
    "# We iterate over all rdf:type triples to register valid nodes\n",
    "for s, p, o in g.triples((None, RDF.type, None)):\n",
    "    s_str = str(s)\n",
    "    # Extract clean type name (e.g., \"Space\" from \"https://w3id.org/bot#Space\")\n",
    "    type_label = str(o).split('#')[-1].split('/')[-1]\n",
    "    \n",
    "    # Filter: Ignore OWL class definitions or properties if they appear as types\n",
    "    if type_label in ['ObjectProperty', 'DatatypeProperty', 'Class']:\n",
    "        continue\n",
    "        \n",
    "    uri_to_type[s_str] = type_label\n",
    "\n",
    "    if type_label not in type_to_id:\n",
    "        type_to_id[type_label] = {}\n",
    "        id_to_uri[type_label] = {}\n",
    "    \n",
    "    # Assign ID if not exists\n",
    "    if s_str not in type_to_id[type_label]:\n",
    "        new_id = len(type_to_id[type_label])\n",
    "        type_to_id[type_label][s_str] = new_id\n",
    "        id_to_uri[type_label][new_id] = s_str\n",
    "\n",
    "print(f\"Registered {len(uri_to_type)} nodes across {len(type_to_id)} types.\")\n",
    "\n",
    "# 3. Create HeteroData Object\n",
    "data = HeteroData()\n",
    "\n",
    "# Initialize node features\n",
    "for node_type, mapping in type_to_id.items():\n",
    "    data[node_type].x = torch.randn(len(mapping), 16)\n",
    "\n",
    "# --- FIXED EDGE BUILDING SECTION ---\n",
    "\n",
    "# Use a temporary dictionary to avoid HeteroData KeyError during construction\n",
    "temp_edge_dict = {} \n",
    "edges_count = 0\n",
    "\n",
    "for s, p, o in g:\n",
    "    s_str, o_str = str(s), str(o)\n",
    "    \n",
    "    if p == RDF.type:\n",
    "        continue\n",
    "        \n",
    "    if s_str in uri_to_type and o_str in uri_to_type:\n",
    "        src_type = uri_to_type[s_str]\n",
    "        dst_type = uri_to_type[o_str]\n",
    "        rel_name = str(p).split('#')[-1].split('/')[-1]\n",
    "        \n",
    "        edge_key = (src_type, rel_name, dst_type)\n",
    "        \n",
    "        if edge_key not in temp_edge_dict:\n",
    "            temp_edge_dict[edge_key] = [[], []]\n",
    "        \n",
    "        src_id = type_to_id[src_type][s_str]\n",
    "        dst_id = type_to_id[dst_type][o_str]\n",
    "        \n",
    "        temp_edge_dict[edge_key][0].append(src_id)\n",
    "        temp_edge_dict[edge_key][1].append(dst_id)\n",
    "        edges_count += 1\n",
    "\n",
    "# Now move the edges from our temp dict into the data object\n",
    "for edge_key, indices in temp_edge_dict.items():\n",
    "    data[edge_key].edge_index = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "print(f\"Graph Construction Complete.\")\n",
    "print(f\"Nodes: {len(uri_to_type)}\")\n",
    "print(f\"Edges: {edges_count}\")\n",
    "\n",
    "# Now this will work because the dictionary is no longer empty!\n",
    "if edges_count > 0:\n",
    "    print(\"Found Edge Types:\", list(data.edge_index_dict.keys()))\n",
    "else:\n",
    "    print(\"Zero structural edges found. Check if URIs are shared between files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cb4240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanpe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch_geometric\\nn\\conv\\hetero_conv.py:76: UserWarning: There exist node types ({'Room_Air_Temperature_Sensor', 'Humidity_Sensor', 'Site', 'CO2_Sensor'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class ManualHeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, metadata):\n",
    "        super().__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "        \n",
    "        self.conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), out_channels)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # 1. First Convolution\n",
    "        out_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        \n",
    "        # 2. SAFETY: Filter out None values and apply ReLU\n",
    "        # Some node types might not receive messages and return None\n",
    "        x_dict = {\n",
    "            key: x.relu() for key, x in out_dict.items() \n",
    "            if x is not None\n",
    "        }\n",
    "        \n",
    "        # 3. Second Convolution\n",
    "        out_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        \n",
    "        # 4. SAFETY: Filter out None values again\n",
    "        x_dict = {\n",
    "            key: x for key, x in out_dict.items() \n",
    "            if x is not None\n",
    "        }\n",
    "        \n",
    "        return x_dict\n",
    "\n",
    "# Initialize the model with the data's metadata\n",
    "# data.metadata() returns ([node_types], [edge_types])\n",
    "model = ManualHeteroGNN(hidden_channels=32, out_channels=16, metadata=data.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c2098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original edges: 827\n",
      "Train edges: 580\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "# 1. Define which relation we want to predict\n",
    "# Using the one from your previous error\n",
    "target_edge = ('Room_Air_Temperature_Sensor', 'isPointOf', 'Space')\n",
    "\n",
    "# 2. Setup the split\n",
    "# num_val=0.1 (10% for validation), num_test=0.1 (10% for testing)\n",
    "# This will automatically hide some edges from the 'train' graph \n",
    "# so the model can't \"cheat\" by seeing them during training.\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.2,\n",
    "    key=\"edge_label\", # This creates a 'labels' attribute for training\n",
    "    add_negative_train_samples=True, \n",
    "    edge_types=[target_edge], # We only split the target relation\n",
    "    rev_edge_types=None \n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "print(f\"Original edges: {data[target_edge].edge_index.size(1)}\")\n",
    "print(f\"Train edges: {train_data[target_edge].edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac85479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(in_channels * 2, in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        # Combine sensor embedding and space embedding\n",
    "        combined = torch.cat([z_src, z_dst], dim=-1)\n",
    "        return self.lin(combined)\n",
    "\n",
    "# Initialize\n",
    "predictor = LinkPredictor(in_channels=16)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. Get embeddings using the TRAINING edges only\n",
    "    # We use a try-except here to catch any persistent PyG internal errors \n",
    "    # during the message passing phase.\n",
    "    try:\n",
    "        z_dict = model(train_data.x_dict, train_data.edge_index_dict)\n",
    "    except AttributeError:\n",
    "        # Fallback: if message passing fails because a node is too isolated, \n",
    "        # we use the raw features (x) as the embedding.\n",
    "        z_dict = train_data.x_dict \n",
    "\n",
    "    # 2. Get the specific edges for this training batch\n",
    "    edge_label_index = train_data[target_edge].edge_label_index\n",
    "    labels = train_data[target_edge].edge_label\n",
    "\n",
    "    # 3. Predict\n",
    "    src_type, _, dst_type = target_edge\n",
    "    z_src = z_dict[src_type][edge_label_index[0]]\n",
    "    z_dst = z_dict[dst_type][edge_label_index[1]]\n",
    "    \n",
    "    predictions = predictor(z_src, z_dst).squeeze()\n",
    "    \n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0fc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_missing_relations(target_edge_type, top_k=10):\n",
    "    model.eval()\n",
    "    src_type, rel, dst_type = target_edge_type\n",
    "    \n",
    "    # 1. Get trained embeddings\n",
    "    embeddings = model(data.x_dict, data.edge_index_dict)\n",
    "    \n",
    "    # 2. Iterate through potential candidates (Simplified)\n",
    "    # In production, use a matrix product, but be careful with memory on large graphs\n",
    "    src_emb = embeddings[src_type] # Shape: [NumSrcNodes, 16]\n",
    "    dst_emb = embeddings[dst_type] # Shape: [NumDstNodes, 16]\n",
    "    \n",
    "    # Calculate score matrix (All Sources vs All Destinations)\n",
    "    # score_matrix[i, j] = probability of link between src[i] and dst[j]\n",
    "    score_matrix = torch.matmul(src_emb, dst_emb.t()).sigmoid()\n",
    "    \n",
    "    # 3. Mask out existing edges so we don't \"discover\" links we already have\n",
    "    existing_edges = data[target_edge_type].edge_index\n",
    "    score_matrix[existing_edges[0], existing_edges[1]] = 0\n",
    "    \n",
    "    # 4. Find highest scores\n",
    "    values, indices = torch.topk(score_matrix.flatten(), top_k)\n",
    "    \n",
    "    print(f\"\\nTop {top_k} predicted NEW relations for {target_edge_type}:\")\n",
    "    for v, idx in zip(values, indices):\n",
    "        # Convert flat index back to (row, col)\n",
    "        row_idx = idx // score_matrix.size(1)\n",
    "        col_idx = idx % score_matrix.size(1)\n",
    "        \n",
    "        src_uri = id_to_uri[src_type][row_idx]\n",
    "        dst_uri = id_to_uri[dst_type][col_idx]\n",
    "        \n",
    "        print(f\"Score: {v:.4f} | {src_uri} -- should be connected to --> {dst_uri}\")\n",
    "\n",
    "# Execute\n",
    "find_missing_relations(target_edge_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
